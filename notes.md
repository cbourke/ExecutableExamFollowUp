

Executable Exam Meeting
2024-10-29

Revisit the taxonomy
 Grading
 Resources allowed (originally: hardware/software)
  maybe expand on specific technologies: text editor, IDE, compiler, linter, dynamic analysis tools (valgrind, debugger)
 AI allowance levels:
   -none
   -filtered AI (codeapp \cite{10.1145/3631802.3631830,10.1145/3636243.3636249}, cs50 integrated: https://cs50.ai/ \cite{Liu2024TeachingCW}, \cite{filteredAI03}
   -unfiltered
 Distinction between AI usage:
   -How do I do X (guideance, likely least allowed)
   -What does X mean (X= term or compiler message)
   -What is wrong with X (troubleshooting)
 Student Perspectives
   -Cheaters will cheat by any means; however, AI may not help (my experience: traditional cheating methods such as soliciting code online/chegg or from others is still just as common) because 1) they don't know how to ask the right questions and 2) cannot discern good solutions from bad (which they then cannot debug)
   -Honest students (honor system); relate this back to Bloom's Taxonomy for CS (understanding and debugging code one did not write is more difficult than just writing it)

SIGCSE Virtual:
 -2024: https://sigcsevirtual.acm.org/program/program-sigcse-virtual-2024/
 -2025: TBD, $100 registration, late May deadline for abstracts

MISC

\cite{10.1145/3689535.3689554} indicates that LLM debugging messages are not all that great
\cite{10.1145/3636515} is a 2024 literature review of automated grading
